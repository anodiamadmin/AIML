Query 1:
-----------------------------
My next assignment is: Part 3 - Japanese Character Recognition
For Part 3 you will be implementing networks to recognize handwritten Hiragana symbols. The dataset to be used is Kuzushiji-MNIST or KMNIST for short. The paper describing the dataset is available here. It is worth reading, but in short: significant changes occurred to the language when Japan reformed their education system in 1868, and the majority of Japanese today cannot read texts published over 150 years ago. This paper presents a dataset of handwritten, labelled examples of this old-style script (Kuzushiji). Along with this dataset, however, they also provide a much simpler one, containing 10 Hiragana characters with 7000 samples per class. This is the dataset we will be using.
modern vs old kuzushiji characters
This part of the assessment is comprised of four steps:

Query 2:
-----------------------------
Manually download the dataset from KMNIST official download page: http://codh.rois.ac.jp/
Download these 4 files (for training and testing):
train-images-idx3-ubyte.gz
train-labels-idx1-ubyte.gz
t10k-images-idx3-ubyte.gz
t10k-labels-idx1-ubyte.gz

Query 3:
-----------------------------
Step 1 [1 mark] - Implement a model NetLin which computes a linear function of the pixels in the image, followed by log softmax. Run the code by typing:

python3 kuzu_main.py --net lin

Query 4:
------------------------------
Step 2 [1 mark] - Implement a fully connected 2-layer network NetFull (i.e. one hidden layer, plus the output layer), using tanh at the hidden layer and log softmax at the output layer. Run the code by typing:
python3 kuzu_main.py --net full
Try different values (multiples of 10) for the number of hidden nodes and try to determine a value that achieves high accuracy (at least 84%) on the test set. Copy the final accuracy and confusion matrix into your report, and include a calculation of the total number of independent parameters in the network.

Query 5:
-----------------------------
Step 2 [1 mark] - Implement a fully connected 2-layer network NetFull (i.e. one hidden layer, plus the output layer), using tanh at the hidden layer and log softmax at the output layer. Run the code by typing:
python3 kuzu_main.py --net full
Try different values (multiples of 10) for the number of hidden nodes and try to determine a value that achieves high accuracy (at least 84%) on the test set. Copy the final accuracy and confusion matrix into your report, and include a calculation of the total number of independent parameters in the network.

after implementing and running the fully connected 2-layer network NetFull

The 10th Epoch outputs look like from running the fillowing command:
python3 kuzu_main.py --net full
with below number of hidden units.

for hidden units = 30
----------------------
Train Epoch: 10 [0/60000 (0%)]  Loss: 0.471437
Train Epoch: 10 [6400/60000 (11%)]      Loss: 0.386906
Train Epoch: 10 [12800/60000 (21%)]     Loss: 0.328778
Train Epoch: 10 [19200/60000 (32%)]     Loss: 0.323985
Train Epoch: 10 [25600/60000 (43%)]     Loss: 0.148746
Train Epoch: 10 [32000/60000 (53%)]     Loss: 0.321856
Train Epoch: 10 [38400/60000 (64%)]     Loss: 0.405924
Train Epoch: 10 [44800/60000 (75%)]     Loss: 0.400084
Train Epoch: 10 [51200/60000 (85%)]     Loss: 0.182335
Train Epoch: 10 [57600/60000 (96%)]     Loss: 0.415828
<class 'numpy.ndarray'>
[[809.   3.   1.   6.  33.  44.   4.  48.  36.  16.]
 [  3. 748.  44.  11.  26.  10.  70.   7.  33.  48.]
 [  7.  26. 810.  34.  20.  15.  37.  10.  22.  19.]
 [  2.  13.  51. 835.   9.  29.  15.  11.  21.  14.]
 [ 60.  36.  41.  13. 725.  14.  30.  13.  26.  42.]
 [  9.  11.  89.  11.  16. 785.  42.   3.  23.  11.]
 [  4.  14. 115.   8.  17.   8. 809.  10.   5.  10.]
 [ 13.  10.  16.   6.  56.  14.  45. 721.  52.  67.]
 [ 14.  27.  33.  32.   3.  20.  30.   4. 825.  12.]
 [  2.  31.  69.   5.  54.  16.  15.  11.  22. 775.]]
Test set: Average loss: 0.7089, Accuracy: 7842/10000 (78%)

for hidden units = 60
----------------------
Train Epoch: 10 [0/60000 (0%)]  Loss: 0.427823
Train Epoch: 10 [6400/60000 (11%)]      Loss: 0.303384
Train Epoch: 10 [12800/60000 (21%)]     Loss: 0.290822
Train Epoch: 10 [19200/60000 (32%)]     Loss: 0.260665
Train Epoch: 10 [25600/60000 (43%)]     Loss: 0.128931
Train Epoch: 10 [32000/60000 (53%)]     Loss: 0.255805
Train Epoch: 10 [38400/60000 (64%)]     Loss: 0.297129
Train Epoch: 10 [44800/60000 (75%)]     Loss: 0.365261
Train Epoch: 10 [51200/60000 (85%)]     Loss: 0.169863
Train Epoch: 10 [57600/60000 (96%)]     Loss: 0.315610
<class 'numpy.ndarray'>
[[844.   4.   3.   6.  35.  23.   5.  46.  27.   7.]
 [  4. 807.  26.   4.  24.  13.  63.   2.  24.  33.]
 [  7.  19. 822.  35.  11.  20.  28.  14.  25.  19.]
 [  6.  14.  33. 887.   6.  17.  13.   3.  11.  10.]
 [ 48.  31.  26.  15. 775.   9.  25.  21.  19.  31.]
 [  8.   9.  64.  11.  13. 849.  20.   2.  16.   8.]
 [  3.  10.  60.   7.  23.   9. 866.   8.   6.   8.]
 [ 11.  10.  22.   6.  39.   9.  43. 788.  32.  40.]
 [ 10.  30.  19.  45.   8.  16.  34.   4. 827.   7.]
 [  2.  19.  45.   5.  35.  14.  25.  17.  13. 825.]]
Test set: Average loss: 0.5655, Accuracy: 8290/10000 (83%)

for hidden units = 80
----------------------
Train Epoch: 10 [0/60000 (0%)]  Loss: 0.400855
Train Epoch: 10 [6400/60000 (11%)]      Loss: 0.309813
Train Epoch: 10 [12800/60000 (21%)]     Loss: 0.237002
Train Epoch: 10 [19200/60000 (32%)]     Loss: 0.205203
Train Epoch: 10 [25600/60000 (43%)]     Loss: 0.114424
Train Epoch: 10 [32000/60000 (53%)]     Loss: 0.245744
Train Epoch: 10 [38400/60000 (64%)]     Loss: 0.253510
Train Epoch: 10 [44800/60000 (75%)]     Loss: 0.391303
Train Epoch: 10 [51200/60000 (85%)]     Loss: 0.150816
Train Epoch: 10 [57600/60000 (96%)]     Loss: 0.239333
<class 'numpy.ndarray'>
[[841.   4.   2.   7.  27.  37.   6.  40.  28.   8.]
 [  5. 809.  35.   3.  20.  14.  67.   3.  14.  30.]
 [  7.  17. 852.  41.   7.  15.  21.  12.  15.  13.]
 [  4.  11.  37. 914.   4.   6.   2.   5.   6.  11.]
 [ 54.  35.  23.   9. 779.  12.  33.  17.  16.  22.]
 [ 11.  16.  91.   8.   9. 812.  28.   3.  16.   6.]
 [  3.  12.  69.   9.  17.   4. 867.   8.   2.   9.]
 [ 13.   7.  18.   7.  35.   8.  38. 807.  29.  38.]
 [ 11.  29.  32.  47.   4.  11.  33.   4. 820.   9.]
 [  2.  28.  65.   1.  38.   4.  19.  16.  10. 817.]]
Test set: Average loss: 0.5470, Accuracy: 8318/10000 (83%)

for hidden units = 90
----------------------
Train Epoch: 10 [0/60000 (0%)]  Loss: 0.379414
Train Epoch: 10 [6400/60000 (11%)]      Loss: 0.237766
Train Epoch: 10 [12800/60000 (21%)]     Loss: 0.282164
Train Epoch: 10 [19200/60000 (32%)]     Loss: 0.230261
Train Epoch: 10 [25600/60000 (43%)]     Loss: 0.143560
Train Epoch: 10 [32000/60000 (53%)]     Loss: 0.316764
Train Epoch: 10 [38400/60000 (64%)]     Loss: 0.259429
Train Epoch: 10 [44800/60000 (75%)]     Loss: 0.343695
Train Epoch: 10 [51200/60000 (85%)]     Loss: 0.134752
Train Epoch: 10 [57600/60000 (96%)]     Loss: 0.294797
<class 'numpy.ndarray'>
[[834.   3.   5.   6.  35.  26.   5.  41.  38.   7.]
 [  4. 801.  36.   5.  16.  16.  60.   6.  24.  32.]
 [  6.  12. 842.  41.  10.  13.  29.  13.  22.  12.]
 [  2.   6.  32. 916.   2.  14.   9.   6.   7.   6.]
 [ 44.  30.  30.   8. 794.   9.  34.  17.  17.  17.]
 [ 10.   9.  85.  10.   8. 824.  27.   2.  21.   4.]
 [  3.  13.  62.   8.  16.   4. 878.   7.   2.   7.]
 [ 20.  14.  19.   5.  32.  11.  30. 806.  28.  35.]
 [ 14.  28.  29.  41.   4.   8.  30.   2. 838.   6.]
 [  3.  15.  56.   4.  25.   7.  27.   9.  17. 837.]]
Test set: Average loss: 0.5391, Accuracy: 8370/10000 (84%)

for hidden units = 100
----------------------
Train Epoch: 10 [0/60000 (0%)]  Loss: 0.361229
Train Epoch: 10 [6400/60000 (11%)]      Loss: 0.276384
Train Epoch: 10 [12800/60000 (21%)]     Loss: 0.241291
Train Epoch: 10 [19200/60000 (32%)]     Loss: 0.234389
Train Epoch: 10 [25600/60000 (43%)]     Loss: 0.121325
Train Epoch: 10 [32000/60000 (53%)]     Loss: 0.242035
Train Epoch: 10 [38400/60000 (64%)]     Loss: 0.264779
Train Epoch: 10 [44800/60000 (75%)]     Loss: 0.383604
Train Epoch: 10 [51200/60000 (85%)]     Loss: 0.132289
Train Epoch: 10 [57600/60000 (96%)]     Loss: 0.292130
<class 'numpy.ndarray'>
[[837.   6.   3.   4.  29.  25.   4.  49.  34.   9.]
 [  4. 810.  27.   6.  23.  14.  63.   6.  17.  30.]
 [  8.  10. 842.  37.  13.  16.  23.  13.  22.  16.]
 [  4.  14.  35. 903.   2.  14.   8.   6.   7.   7.]
 [ 49.  33.  20.   9. 791.   9.  31.  21.  20.  17.]
 [  9.  10.  81.  13.   7. 822.  34.   3.  14.   7.]
 [  3.  10.  55.   9.  15.   8. 877.  12.   4.   7.]
 [ 21.  13.  19.   3.  21.  14.  35. 819.  26.  29.]
 [  9.  25.  36.  45.   6.  14.  26.   4. 829.   6.]
 [  5.  15.  55.   3.  31.   9.  28.  14.  18. 822.]]
Test set: Average loss: 0.5365, Accuracy: 8352/10000 (84%)

for hidden units = 110
----------------------
Train Epoch: 10 [0/60000 (0%)]  Loss: 0.407157
Train Epoch: 10 [6400/60000 (11%)]      Loss: 0.239276
Train Epoch: 10 [12800/60000 (21%)]     Loss: 0.264281
Train Epoch: 10 [19200/60000 (32%)]     Loss: 0.195745
Train Epoch: 10 [25600/60000 (43%)]     Loss: 0.105638
Train Epoch: 10 [32000/60000 (53%)]     Loss: 0.241209
Train Epoch: 10 [38400/60000 (64%)]     Loss: 0.231218
Train Epoch: 10 [44800/60000 (75%)]     Loss: 0.343336
Train Epoch: 10 [51200/60000 (85%)]     Loss: 0.147808
Train Epoch: 10 [57600/60000 (96%)]     Loss: 0.239638
<class 'numpy.ndarray'>
[[842.   4.   1.   9.  32.  31.   4.  36.  33.   8.]
 [  6. 811.  34.   3.  20.   7.  68.   7.  16.  28.]
 [  7.  17. 825.  37.  11.  22.  29.  18.  20.  14.]
 [  4.  10.  39. 906.   1.  12.   9.   3.   8.   8.]
 [ 50.  34.  23.   7. 793.   7.  31.  15.  17.  23.]
 [  7.  11.  71.   9.  10. 841.  31.   1.  12.   7.]
 [  3.  15.  47.   8.  18.  11. 881.   8.   2.   7.]
 [ 21.  10.  19.   3.  26.   9.  41. 818.  23.  30.]
 [  8.  36.  29.  46.   5.  14.  34.   3. 814.  11.]
 [  2.  23.  53.   2.  33.   6.  27.  12.  10. 832.]]
Test set: Average loss: 0.5290, Accuracy: 8363/10000 (84%)

for hidden units = 120
----------------------
Train Epoch: 10 [0/60000 (0%)]  Loss: 0.366012
Train Epoch: 10 [6400/60000 (11%)]      Loss: 0.258515
Train Epoch: 10 [12800/60000 (21%)]     Loss: 0.256561
Train Epoch: 10 [19200/60000 (32%)]     Loss: 0.224264
Train Epoch: 10 [25600/60000 (43%)]     Loss: 0.117196
Train Epoch: 10 [32000/60000 (53%)]     Loss: 0.256290
Train Epoch: 10 [38400/60000 (64%)]     Loss: 0.203329
Train Epoch: 10 [44800/60000 (75%)]     Loss: 0.356674
Train Epoch: 10 [51200/60000 (85%)]     Loss: 0.122782
Train Epoch: 10 [57600/60000 (96%)]     Loss: 0.277172
<class 'numpy.ndarray'>
[[852.   3.   1.   8.  28.  32.   5.  39.  28.   4.]
 [  6. 808.  32.   3.  23.  12.  67.   6.  16.  27.]
 [  6.  18. 839.  43.  14.  15.  22.   9.  16.  18.]
 [  4.  10.  35. 910.   1.  16.   7.   3.   4.  10.]
 [ 42.  27.  25.   9. 803.   9.  34.  14.  17.  20.]
 [  8.   9.  88.  13.  12. 827.  22.   3.  10.   8.]
 [  2.  15.  61.   6.  17.   5. 878.   7.   1.   8.]
 [ 22.  11.  26.   7.  27.  10.  34. 815.  24.  24.]
 [ 12.  33.  21.  53.   5.   9.  36.   6. 817.   8.]
 [  4.  20.  47.   8.  35.   6.  22.  13.  12. 833.]]
Test set: Average loss: 0.5290, Accuracy: 8382/10000 (84%)

for hidden units = 140
----------------------
Train Epoch: 10 [0/60000 (0%)]  Loss: 0.390009
Train Epoch: 10 [6400/60000 (11%)]      Loss: 0.300705
Train Epoch: 10 [12800/60000 (21%)]     Loss: 0.222767
Train Epoch: 10 [19200/60000 (32%)]     Loss: 0.217322
Train Epoch: 10 [25600/60000 (43%)]     Loss: 0.132990
Train Epoch: 10 [32000/60000 (53%)]     Loss: 0.289680
Train Epoch: 10 [38400/60000 (64%)]     Loss: 0.184732
Train Epoch: 10 [44800/60000 (75%)]     Loss: 0.347844
Train Epoch: 10 [51200/60000 (85%)]     Loss: 0.134134
Train Epoch: 10 [57600/60000 (96%)]     Loss: 0.275880
<class 'numpy.ndarray'>
[[837.   3.   1.   6.  31.  32.   4.  44.  34.   8.]
 [  5. 812.  34.   5.  20.  11.  60.   6.  19.  28.]
 [  8.  14. 830.  43.  12.  18.  27.   8.  21.  19.]
 [  3.   9.  32. 911.   2.  17.   6.   4.   9.   7.]
 [ 46.  31.  20.   4. 809.   9.  29.  16.  22.  14.]
 [  7.   9.  72.   7.  13. 849.  24.   1.  12.   6.]
 [  4.  12.  67.   7.  17.   7. 874.   6.   1.   5.]
 [ 18.  15.  24.   5.  33.  11.  30. 807.  27.  30.]
 [  8.  28.  33.  44.   2.   9.  30.   4. 836.   6.]
 [  3.  14.  56.   3.  28.   6.  18.  11.  11. 850.]]
Test set: Average loss: 0.5223, Accuracy: 8415/10000 (84%)

Give me a writeup for my report.docx/pdf

Query 6:
----------------------------------------------
Implement a convolutional network called NetConv, with two convolutional layers plus one fully connected layer, all using relu activation function, followed by the output layer, using log softmax. You are free to choose for yourself the number and size of the filters, meta parameter values (learning rate and momentum), and whether to use max pooling or a fully convolutional architecture. Run the code by typing:
python3 kuzu_main.py --net conv 
Your network should consistently achieve at least 93% accuracy on the test set after 10 training epochs. Copy the final accuracy and confusion matrix into your report, and include a calculation of the total number of independent parameters in the network.

Query 7:
----------------------------------------------
Suggested Training Settings
Optimizer: torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
Epochs: 10
Batch size: 64
Loss: nn.NLLLoss()
Normalize input with: transforms.Normalize((0.5,), (0.5,))
How do I implement this?
-----------------------------------------------
Query 8:
Step 3 - [2 Mark] Implement a convolutional network called NetConv, with two convolutional layers plus one fully connected layer, all using relu activation function, followed by the output layer, using log softmax. You are free to choose for yourself the number and size of the filters, meta parameter values (learning rate and momentum), and whether to use max pooling or a fully convolutional architecture. Run the code by typing:

python3 kuzu_main.py --net conv 

Your network should consistently achieve at least 93% accuracy on the test set after 10 training epochs. Copy the final accuracy and confusion matrix into your report, and include a calculation of the total number of independent parameters in the network.
ChatGPT already provided the code for the convolutional neural network.
after running the given code with the following command:
python3 kuzu_main.py --net conv 
We can get the fillowing output (only last few epochs are given below)

Train Epoch: 7 [0/60000 (0%)]   Loss: 0.056176
Train Epoch: 7 [6400/60000 (11%)]       Loss: 0.032908
Train Epoch: 7 [12800/60000 (21%)]      Loss: 0.083040
Train Epoch: 7 [19200/60000 (32%)]      Loss: 0.041646
Train Epoch: 7 [25600/60000 (43%)]      Loss: 0.016163
Train Epoch: 7 [32000/60000 (53%)]      Loss: 0.078414
Train Epoch: 7 [38400/60000 (64%)]      Loss: 0.040491
Train Epoch: 7 [44800/60000 (75%)]      Loss: 0.150555
Train Epoch: 7 [51200/60000 (85%)]      Loss: 0.025905
Train Epoch: 7 [57600/60000 (96%)]      Loss: 0.060678
<class 'numpy.ndarray'>
[[923.   6.   1.   1.  33.   6.   4.  15.   7.   4.]
 [  1. 901.   8.   1.  12.   1.  51.   8.   8.   9.]
 [  6.   8. 890.  34.  14.   9.  12.   5.   6.  16.]
 [  1.   1.  11. 965.   2.   3.   7.   4.   1.   5.]
 [ 11.   6.   4.  10. 930.   2.  16.   4.  12.   5.]
 [  3.   7.  49.   5.   5. 896.  23.   1.   3.   8.]
 [  2.   3.  13.   2.   9.   1. 964.   4.   1.   1.]
 [  1.   6.   3.   0.   6.   1.  10. 953.   7.  13.]
 [  1.  10.  10.   9.   6.   6.  13.   3. 939.   3.]
 [  9.   3.  13.   2.  15.   0.  13.   9.  19. 917.]]
Test set: Average loss: 0.2883, Accuracy: 9278/10000 (93%)

Train Epoch: 8 [0/60000 (0%)]   Loss: 0.036666
Train Epoch: 8 [6400/60000 (11%)]       Loss: 0.021964
Train Epoch: 8 [12800/60000 (21%)]      Loss: 0.050350
Train Epoch: 8 [19200/60000 (32%)]      Loss: 0.032992
Train Epoch: 8 [25600/60000 (43%)]      Loss: 0.011534
Train Epoch: 8 [32000/60000 (53%)]      Loss: 0.061394
Train Epoch: 8 [38400/60000 (64%)]      Loss: 0.027423
Train Epoch: 8 [44800/60000 (75%)]      Loss: 0.105059
Train Epoch: 8 [51200/60000 (85%)]      Loss: 0.020610
Train Epoch: 8 [57600/60000 (96%)]      Loss: 0.047273
<class 'numpy.ndarray'>
[[928.   4.   1.   1.  34.   5.   3.  13.   6.   5.]
 [  1. 903.   9.   1.  11.   1.  49.   9.   8.   8.]
 [  6.   8. 898.  33.  13.   7.  12.   4.   5.  14.]
 [  1.   0.   9. 966.   3.   4.   7.   4.   1.   5.]
 [ 12.   6.   4.  11. 931.   1.  15.   3.  12.   5.]
 [  4.   6.  57.   5.   4. 884.  26.   2.   4.   8.]
 [  2.   2.  15.   2.  11.   0. 963.   3.   1.   1.]
 [  0.   3.   2.   0.   8.   1.  12. 952.   8.  14.]
 [  1.   6.  11.   7.   9.   3.  13.   3. 944.   3.]
 [  9.   2.  16.   2.  15.   1.  12.   9.  16. 918.]]
Test set: Average loss: 0.2963, Accuracy: 9287/10000 (93%)

Train Epoch: 9 [0/60000 (0%)]   Loss: 0.025890
Train Epoch: 9 [6400/60000 (11%)]       Loss: 0.016723
Train Epoch: 9 [12800/60000 (21%)]      Loss: 0.029122
Train Epoch: 9 [19200/60000 (32%)]      Loss: 0.023052
Train Epoch: 9 [25600/60000 (43%)]      Loss: 0.009952
Train Epoch: 9 [32000/60000 (53%)]      Loss: 0.039334
Train Epoch: 9 [38400/60000 (64%)]      Loss: 0.016948
Train Epoch: 9 [44800/60000 (75%)]      Loss: 0.070471
Train Epoch: 9 [51200/60000 (85%)]      Loss: 0.013974
Train Epoch: 9 [57600/60000 (96%)]      Loss: 0.030187
<class 'numpy.ndarray'>
[[931.   5.   1.   1.  32.   6.   2.  12.   6.   4.]
 [  2. 902.   6.   1.  12.   1.  48.   9.   9.  10.]
 [  6.   6. 903.  31.  12.   7.  12.   4.   4.  15.]
 [  1.   0.   9. 968.   3.   4.   6.   4.   1.   4.]
 [ 12.   7.   4.  10. 929.   0.  16.   5.  12.   5.]
 [  3.   8.  45.   6.   4. 894.  25.   3.   5.   7.]
 [  1.   2.  12.   4.   9.   0. 966.   4.   1.   1.]
 [  1.   2.   2.   0.   8.   1.  12. 954.   8.  12.]
 [  1.   8.   9.   8.   9.   3.  13.   3. 944.   2.]
 [ 10.   1.  16.   3.  15.   1.  12.   8.  16. 918.]]
Test set: Average loss: 0.2998, Accuracy: 9309/10000 (93%)

Train Epoch: 10 [0/60000 (0%)]  Loss: 0.017127
Train Epoch: 10 [6400/60000 (11%)]      Loss: 0.010254
Train Epoch: 10 [12800/60000 (21%)]     Loss: 0.014605
Train Epoch: 10 [19200/60000 (32%)]     Loss: 0.011647
Train Epoch: 10 [25600/60000 (43%)]     Loss: 0.005436
Train Epoch: 10 [32000/60000 (53%)]     Loss: 0.034138
Train Epoch: 10 [38400/60000 (64%)]     Loss: 0.011004
Train Epoch: 10 [44800/60000 (75%)]     Loss: 0.049022
Train Epoch: 10 [51200/60000 (85%)]     Loss: 0.008953
Train Epoch: 10 [57600/60000 (96%)]     Loss: 0.016530
<class 'numpy.ndarray'>
[[935.   6.   1.   1.  29.   5.   1.  11.   6.   5.]
 [  3. 906.   8.   1.  10.   1.  45.  10.   8.   8.]
 [  6.   6. 907.  30.  12.   9.  10.   3.   4.  13.]
 [  1.   0.  12. 967.   3.   4.   4.   4.   1.   4.]
 [ 17.   7.   4.  12. 925.   0.  15.   5.  13.   2.]
 [  3.   8.  44.   6.   4. 895.  24.   3.   4.   9.]
 [  2.   2.  15.   5.   7.   0. 963.   4.   1.   1.]
 [  3.   3.   3.   0.   6.   2.  10. 952.   8.  13.]
 [  2.   7.  10.   7.   8.   3.  11.   1. 950.   1.]
 [ 11.   1.  14.   4.  14.   0.  13.   8.  16. 919.]]
Test set: Average loss: 0.3071, Accuracy: 9319/10000 (93%)

Give a writeup for the report.pdf.
---------------------------------------------
Query 9:
Give me a solid writeup for the below questions for my report.pdf:
Step 4 [4 marks] - Briefly discuss the following points:
1. the relative accuracy of the three models,[1 mark]
2. the number of independent parameters in each of the three models,[1 mark]
3. the confusion matrix for each model: which characters are most likely to be mistaken for which other characters, and why?[2 marks]


Query 10:
--------------------------------------------
Number of parameters for the Convolutional Neural Network is calculated once as:
 - Conv1: (10 × 5 × 5 × 1) + 10 = 260
 - Conv2: (20 × 5 × 5 × 10) + 20 = 5,020
 - Flattened conv output: 4 × 4 × 20 = 320 → Fully connected: (320 + 1) × 10 = 3,210
 = Total = 260 + 5,020 + 3,210 = 8,490 parameters

In other place as:
The total number of independent parameters in the network was calculated by summing the weights and biases of all layers:
First Convolutional Layer (Conv1): 10 filters, size 5x5, input channels = 1
 - Number of weights and biases = (10 × 1 × 5 × 5) + 10 = 260
Second Convolutional Layer (Conv2): 20 filters, size 5x5, input channels = 10
 - Number of weights and biases = (20 × 10 × 5 × 5) + 20 = 5020
Fully Connected Layer (FC1): input size 320, output = 50
 - Number of weights and biases = (320 × 50) + 50 = 16050
Output Layer: input size 50, output = 10
 - Number of weights and biases = (50 × 10) + 10 = 510
Total Parameters = 260 + 5020 + 16050 + 510 = 21840

The code is:
class NetConv(nn.Module):
    # two convolutional layers and one fully connected layer,
    # all using relu, followed by log_softmax
    def __init__(self):
        # super(NetConv, self).__init__()
        # INSERT CODE HERE
        super(NetConv, self).__init__()
        # First conv layer: 1 input channel, 32 output channels
        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)  # 28x28 → 28x28
        # Second conv layer: 32 input channels, 64 output channels
        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)  # 28x28 → 28x28
        # Max pooling layer
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)  # 28x28 → 14x14
        # Fully connected layer
        self.fc1 = nn.Linear(64 * 14 * 14, 128)
        self.out = nn.Linear(128, 10)

    def forward(self, x):
        # return 0 # CHANGE CODE HERE
        x = F.relu(self.conv1(x))  # (B,32,28,28)
        x = F.relu(self.conv2(x))  # (B,64,28,28)
        x = self.pool(x)  # (B,64,14,14)
        x = x.view(x.size(0), -1)  # flatten (B, 64*14*14)
        x = F.relu(self.fc1(x))  # (B,128)
        x = self.out(x)  # (B,10)
        return F.log_softmax(x, dim=1)
