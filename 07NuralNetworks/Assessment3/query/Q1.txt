This is about: Validation set was used correctly with clear explanation of how it was used.

General advice:
Use the variable train_val_split to help you make design decisions aimed to avoid overfitting to the training data. At the very end, you may wish to re-train using the entire training set.
Try to be methodical in your development. Blindly modifying code, looking at the output, then modifying again can cause you go around in circles. A better approach is to keep a record of what you have tried, and what outcome you observed. Decide on a hypothesis you want to test, run an experiment and record the result. Then move on to the next idea

The previous version of the report's "Validation Set Usage" section says the following:
-----------------
Although the project specification allows for train_val_split = 1 (training on all available images), during experimentation, a temporary hold-out validation split (e.g., 95:5) was created to tune learning rate, dropout rate, stochastic depth probability, and augmentation parameters. This separation ensured that all optimization decisions were informed by unseen data rather than the training set, thereby minimizing the risk of overfitting.
Metrics computed on the validation set included:
 - Top-1 classification accuracy
 - Cross-entropy loss trend
 - Confusion matrix to identify breed-specific misclassifications
By monitoring these metrics across epochs, optimal stopping points and scheduler configurations were identified before final training on the full dataset.
--------------------
I have 8 classes of cats with 1000 images of each. I took out 50 images from each class for validation (400 validation images). I trained with remaining 950 images from each class (15600 training images).

When I used train_val_split = 0.8 and trained for 40 epochs.
The output from a3main.py showed consistant increase of both training and test accuracies and both went greater than 80-85% by the 40th epoch.

After training with train_val_split = 0.8 for 40 epochs, the output from validation.py on the unseen vaidation-set (400 heldout images, 50images from each of the 8 classes) is as follows:
PS D:\anodiam\AIML\07NuralNetworks\Assessment3\a3Validation> python validation.py
Validated on 400 images
Overall accuracy: 83.75%

Confusion matrix (rows = true, cols = predicted):
[[45  1  0  1  0  0  2  1]
 [ 0 38  0  2  4  0  4  2]
 [ 0  1 34  5  3  3  1  3]
 [ 2  0  2 43  0  1  1  1]
 [ 0  1  2  2 43  0  0  2]
 [ 0  1  0  0  0 48  1  0]
 [ 0  2  1  1  0  4 41  1]
 [ 3  2  0  0  0  0  2 43]]

Per-class accuracy:
  Class 0: 90.00%  (45/50)
  Class 1: 76.00%  (38/50)
  Class 2: 68.00%  (34/50)
  Class 3: 86.00%  (43/50)
  Class 4: 86.00%  (43/50)
  Class 5: 96.00%  (48/50)
  Class 6: 82.00%  (41/50)
  Class 7: 86.00%  (43/50)
-----------------------------------------
Next I used the same 400 validation images and 15600 training images, but train_val_split = 1 and trained for 40 epochs.

The output from validation.py on the unseen vaidation-set (400 heldout images, 50images from each of the 8 classes) is as follows:
Validated on 400 images
Overall accuracy: 82.00%

Confusion matrix (rows = true, cols = predicted):
[[43  0  0  4  0  0  1  2]
 [ 0 42  1  0  1  1  4  1]
 [ 0  0 39  1  1  5  1  3]
 [ 4  1  0 43  0  1  1  0]
 [ 0  3  8  3 34  1  0  1]
 [ 0  0  0  0  0 50  0  0]
 [ 0  4  0  3  0  6 37  0]
 [ 2  6  0  1  0  0  1 40]]

Per-class accuracy:
  Class 0: 86.00%  (43/50)
  Class 1: 84.00%  (42/50)
  Class 2: 78.00%  (39/50)
  Class 3: 86.00%  (43/50)
  Class 4: 68.00%  (34/50)
  Class 5: 100.00%  (50/50)
  Class 6: 74.00%  (37/50)
  Class 7: 80.00%  (40/50)

----------------------------------------

Rewrite the report's "Validation Set Usage" section as per the above findings.